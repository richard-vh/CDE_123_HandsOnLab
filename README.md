# CDE 1.23 Hands On Lab

## About the CDE Lab

In these Labs you will gain hands-on experience the Cloudera Data Engineering Service. In this excercise you will:

1. Develop a PySpark and Iceberg Application with Spark 3.5 connecting to a Spark Virtual Cluster via Spark Connect using the VSCode IDE.
2. Prototype your Spark Application in a dedicated CDE DEV environment.
3. Use git to back up your code and push it to a dedicated CDE PRD environment.
4. Create an Airflow Pipeline to orchestrate multiple Spark Applications.
5. Monitor your Spark Applications with CDP Observability and the CDP Data Catalog.

## About the Cloudera Data Engineering (CDE) Service

CDE is the Cloudera Data Engineering Service, a containerized managed service for Cloudera Data Platform designed for Large Scale Batch Pipelines with Spark, Airflow and Iceberg. It allows you to submit batch jobs to auto-scaling virtual clusters. As a Cloud-Native service, CDE enables you to spend more time on your applications, and less time on infrastructure.

CDE allows you to create, manage, and schedule Apache Spark jobs without the overhead of creating and maintaining Spark clusters. With CDE, you define virtual clusters with a range of CPU and memory resources, and the cluster scales up and down as needed to run your Spark workloads, helping to control your cloud costs.

## Step by Step Instructions

Detailed instructions are provided in the [step_by_step_guides](https://github.com/pdefusco/CDE_123_HOL/tree/main/step_by_step_guides/english) folder.

* [Link to the English Guide]().
* [Enlace a la Guía en Español]()
* [Lien vers le Guide en Français]()
* [Link per la Guida in Italiano]()
* [Link para o Guia em Português]()

## Setup Instructions

The HOL requires data and CDE dependencies (e.g. shared Files, Pyhton, Docker Resource) to be created before the event. The attached [Setup Guide]() provides instructions for meeting these requirements.
